# project-house-price-prediction
This project uses [Kaggle's house prices dataset](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data) to apply various predictive models, including regression, ensemble, and boosting, for continuous target. **For each model, an introduction, the algorithm, and the objective function are discussed in the notebook along with Python codes.** The goals of this project are to:
- perform data exploration and build standard sklearn estimator for data transform
- comprehend popular predictive models through discussions on algorithms and objectives
- connect mathmetical algorithms with codes/parameters in standard Python packages 

Eventually, this project serves the purpose of building the fundation for other advanced machine learning projects encountered in the future.

There are three main workbooks, each owns a different topic. 
- **Data Exploration & Preparation:** data-exploration-and-preprocessing
- **Regression and Ensemble Models:** models
- **Boosting Models:** AdaBoost-GBM-XGBoost

## Overview

#### Regression
1. Linear Regression
    - Probablistic View of Linear Regression
2. Ridge Regression
    - Probablisitc View: Ridge Regression and Bayesian Modeling
3. Sparse Regression (Lasso)
4. Elastic Net
    - Model Comparison - Linear Regression, Ridge, and Lasso
5. Kernel Ridge
    - Kernels
6. Regression Tree

#### Ensemble
1. Bagging
    - Algorithm
    - Model Improvement
2. Random Forest
    - Algorithm
3. AdaBoost
    - Bagging vs AdaBoosting
    - Algorithm (Regression)
4. Gradient Boosting Machine (GBM)
    - AdaBoost vs GBM
    - General Form
    - Algorithm
5. XGBoost
    - GBM vs XGBoost
    - Algorithm
    
    
