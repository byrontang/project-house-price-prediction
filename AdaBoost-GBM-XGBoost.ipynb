{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# House Price Prediction\n",
    "\n",
    "\n",
    "## Boosting\n",
    "\n",
    "This notebook discusses the algorithm and application of boosting methodologies for better predicting performance than single models.\n",
    "\n",
    "## Overview\n",
    "- AdaBoost\n",
    "- Gradient Boosting\n",
    "- XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read in the data\n",
    "df = pd.read_csv('../project-house-price-prediction/data/train.csv')\n",
    "df_test = pd.read_csv('../project-house-price-prediction/data/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "The data preprocessing follows the data exploration done in the notebook notebook data-exploration-and-preprocessing and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CleanHouseAttributes(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Apply rules during data exploration to clean house price dataset\"\"\"\n",
    "    \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df_house_price, cols_to_drop, target_col):\n",
    "        # Age of building/remodle from YearBuilt and YearRemodAdd\n",
    "        df_house_price['AgeBuilding'] = 2012 - df_house_price['YearBuilt']\n",
    "        df_house_price['AgeRemodel'] = 2012 - df_house_price['YearRemodAdd']\n",
    "    \n",
    "        # Remove categories not exist in test.csv\n",
    "        df_filtered = df_house_price[(df_house_price['HouseStyle'] != '2.5Fin') &\n",
    "                                     (df_house_price['Exterior1st'] != 'Stone') &\n",
    "                                     (df_house_price['Exterior1st'] != 'ImStucc') &\n",
    "                                     (df_house_price['Exterior2nd'] != 'Other')]\n",
    "        \n",
    "        # Drop columns\n",
    "        df_dropped = df_filtered.drop(cols_to_drop + target_col, axis=1)\n",
    "            \n",
    "        # Fill NA for numeric columns\n",
    "        df_numeric = df_dropped.select_dtypes(include=['int64', 'float64']).apply(lambda x: x.fillna(x.mean()), axis=1)\n",
    "        \n",
    "        # Create dummies for non numeric columns\n",
    "        df_nonNumeric = pd.get_dummies(df_dropped.select_dtypes(include=['object']).fillna('NA'))\n",
    "        \n",
    "        # Create boolean variables for Alley, PoolQC, and Fence\n",
    "        df['HasAlley'] = list(1 if x is None else 0 for x in df['Alley'])\n",
    "        df['HasPool'] = list(1 if x is None else 0 for x in df['PoolQC'])\n",
    "        df['Fence'] = list(1 if x is None else 0 for x in df['Fence'])\n",
    "        \n",
    "        X = pd.concat([df_numeric, df_nonNumeric], axis=1)\n",
    "        y = df_filtered[target_col]\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['Id', 'YearBuilt', 'YearRemodAdd', 'GarageYrBlt', 'TotalBsmtSF', \n",
    "             'TotRmsAbvGrd', 'MoSold', 'YrSold', 'Street', 'Alley', 'Utilities', \n",
    "             'LandSlope', 'Condition2', 'Heating', 'Functional', 'FireplaceQu', \n",
    "             'PoolQC', 'Fence', 'MiscFeature'            ]\n",
    "target_col = ['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoosting\n",
    "\n",
    "In bagging, each training example is equally likely to be picked. In boosting, the probability of a particular example being in the training set of a particular machine depends on the performance of the prior machines on that sample. In each round, each training samples is assigned a new weight depending on the prediction performance from the previous round. More weights are assigned to samples with worse prediction so that each subsequent machine would focus on training the \"difficult\" samples.\n",
    "\n",
    "#### Algorithm\n",
    "Given $(x_1, y_1),...,(x_n, y_n)$, assign a weight $w_i=1$ for $i = 1,...,n$.\n",
    "\n",
    "For $t = 1,..., T$\n",
    "1. The probability that training sample $i$ is in the training set is $p_i = \\frac{w_i}{\\sum w_i}$ where the summation is over all members of the training set. Pick $n$ samples with replacement to form the training set.\n",
    "2. Construct a regression machine $t$.\n",
    "3. Make prediction $y_i^{(p)}(x_i)$ for $i=1,...,n$ with machine $t$. **Note: $y_i^{(p)}(x_i)$ is not the final prediction.**\n",
    "4. Calculate a loss for $y_i^{(p)}(x_i)$ and $y_i$. The loss function may be of any functional form as long as $L \\in$ [0,1]. If we let $$ D = sup|y_i^{(p)}(x_i)-y_i|  i = 1,...,n,$$ which means D is the largest error, then we have three candidate loss functions:\n",
    "$$L_i=\\frac{|y_i^{(p)}(x_i)-y_i|}{D} \\textit{(linear)}$$\n",
    "$$L_i=\\frac{|y_i^{(p)}(x_i)-y_i|^2}{D^2} \\textit{(square law)}$$\n",
    "$$L_i=1-\\exp\\left[\\frac{-|y_i^{(p)}(x_i)-y_i|}{D}\\right] \\textit{(exponential)}$$\n",
    "5. Calculate aveage loss $\\overline{L}=\\sum_{i=1}^{N_1}L_ip_i$\n",
    "6. Form $\\beta=\\frac{\\overline{L}}{1-\\overline{L}}$. $\\beta$ is a measure of confidence in the predictor. Low $\\beta$ means high confidence in the prediction.\n",
    "7. Update the weights: $w_i \\rightarrow w_i\\beta^{[1-L_i]}$. The smaller the loss, the more weight is reduced , making the sample less likely to be picked in the next round.\n",
    "8. For a particular input $x_i$, each of the $T$ machines makes a prediction $h_t, t=1,...,T$. $h_f$ is cumulative prediction using the $T$ predictors:\n",
    "$$h_f = inf\\bigg\\{y \\in Y: \\sum_{t:h_t\\leq y}log(\\frac{1}{\\beta_t})\\geq\\frac{1}{2}\\sum_{t}log(\\frac{1}{\\beta_t})\\bigg\\}$$\n",
    "This equation is essentially relabel $y_i$ such that $y_i^{(1)}<y_i^{(2)}<,...,y_i^{(T)}$. Then sum the $log(1/\\beta_t)$ until we reach the smallest t that is equal or grater than $\\frac{1}{2}\\sum_{t}log(\\frac{1}{\\beta_t})$. This is the weighted median. If the $\\beta_t$ were all equal, this would be the median.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "abr = AdaBoostRegressor(DecisionTreeRegressor(max_depth=20), \n",
    "                        n_estimators=100,\n",
    "                        loss='exponential')\n",
    "abr.fit(X_train, np.ravel(y_train))\n",
    "y_pred = abr.predict(X_test)\n",
    "print(abr.score(X_test, y_test), np.sqrt(mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'n_estimators': [100, 250, 500]}\n",
    "abr = AdaBoostRegressor(DecisionTreeRegressor(max_depth=10))\n",
    "gs_cv = GridSearchCV(abr, param_grid, cv = 5)\n",
    "gs_cv.fit(X_train, np.ravel(y_train))\n",
    "print(gs_cv.best_params_, gs_cv.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr = GradientBoostingRegressor(learning_rate = 0.01,\n",
    "                                n_estimators = 1000,\n",
    "                                max_depth = 10)\n",
    "gbr.fit(X_train, np.ravel(y_train))\n",
    "y_pred = gbr.predict(X_test)\n",
    "print(gbr.score(X_test, y_test), np.sqrt(mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'n_estimators': [100, 150],\n",
    "              'learning_rate': np.linspace(0.1, 0.2, 10)}\n",
    "gbr = GradientBoostingRegressor(max_depth=8, max_features='sqrt')\n",
    "gs_cv = GridSearchCV(estimator=gbr, param_grid=param_grid, cv=5)\n",
    "gs_cv.fit(X_train, np.ravel(y_train))\n",
    "print(gs_cv.best_params_, gs_cv.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance on Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_kaggle, y_test_kaggle = house_price_data_cleaning(df_test, drop_cols)\n",
    "lr_kaggle = LinearRegression()\n",
    "lr_kaggle.fit(X, y)\n",
    "y_pred_kaggle = lr_kaggle.predict(X_test_kaggle)\n",
    "y_pred_kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "- H. Drucker, “Improving Regressors using Boosting Techniques”, 1997.\n",
    "- T. Hastie, R. Tibshirani and J. Friedman. Elements of Statistical Learning Ed. 2, Springer, 2009."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
