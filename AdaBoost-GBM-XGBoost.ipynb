{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# House Price Prediction\n",
    "\n",
    "\n",
    "## Boosting\n",
    "\n",
    "This notebook discusses the algorithm and application of boosting methodologies for better predicting performance compared to single models.\n",
    "\n",
    "## Overview\n",
    "- AdaBoost\n",
    "    - Bagging vs AdaBoosting\n",
    "    - Algorithm (Regression)\n",
    "- Gradient Boosting Machine (GBM)\n",
    "    - AdaBoost vs GBM\n",
    "    - General Form\n",
    "    - Algorithm\n",
    "- XGBoost\n",
    "    - GBM vs XGBoost\n",
    "    - Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read in the data\n",
    "df = pd.read_csv('../project-house-price-prediction/data/train.csv')\n",
    "df_test = pd.read_csv('../project-house-price-prediction/data/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "The data preparation below follows the data exploration done in the notebook data-exploration-and-preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CleanHouseAttributes(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Apply rules during data exploration to clean house price dataset\"\"\"\n",
    "    \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df_house_price, cols_to_drop, target_col=[]):\n",
    "        # Age of building/remodle from YearBuilt and YearRemodAdd\n",
    "        df_house_price['AgeBuilding'] = 2012 - df_house_price['YearBuilt']\n",
    "        df_house_price['AgeRemodel'] = 2012 - df_house_price['YearRemodAdd']\n",
    "        \n",
    "        # Create boolean variables for Alley, PoolQC, and Fence\n",
    "        df_house_price['HasAlley'] = ~df_house_price[['Alley']].isnull()\n",
    "        df_house_price['HasPool'] = ~df_house_price[['PoolQC']].isnull()\n",
    "        df_house_price['Fence'] = ~df_house_price[['Fence']].isnull()\n",
    "\n",
    "        # Remove categories not exist in test.csv\n",
    "        df_filtered = df_house_price[(df_house_price['HouseStyle'] != '2.5Fin') &\n",
    "                                     (~df_house_price['RoofMatl'].isin(['Membran', 'Roll', 'ClyTile', 'Metal'])) &\n",
    "                                     (~df_house_price['Exterior1st'].isin(['Stone', 'ImStucc'])) &\n",
    "                                     (df_house_price['Exterior2nd'] != 'Other') &\n",
    "                                     (df_house_price['Electrical'] != 'Mix')]\n",
    "        \n",
    "        # Drop columns\n",
    "        df_dropped = df_filtered.drop(cols_to_drop + target_col + ['HasAlley', 'HasPool', 'Fence'], axis=1)\n",
    "               \n",
    "        # Fill NA for numeric columns\n",
    "        df_numeric = df_dropped.select_dtypes(include=['int64', 'float64']).apply(lambda x: x.fillna(x.mean()), axis=1)\n",
    "        \n",
    "        # Fill NA for non-numcric columns with the most frequent category\n",
    "        df_nonNumeric = df_dropped.select_dtypes(include=['object']).apply(lambda x: x.fillna(x.mode()[0]))\n",
    "                \n",
    "        X = pd.concat([df_numeric, pd.get_dummies(df_nonNumeric)], axis=1)\n",
    "        y = df_filtered[target_col]\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['Id', 'YearBuilt', 'YearRemodAdd', 'GarageYrBlt', 'TotalBsmtSF', \n",
    "             'TotRmsAbvGrd', 'MoSold', 'YrSold', 'GarageQual', 'Street', 'Alley', \n",
    "             'Utilities', 'LandSlope', 'Condition2', 'Heating', 'Functional', \n",
    "             'FireplaceQu', 'PoolQC', 'Fence', 'MiscFeature']\n",
    "target_col = ['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = CleanHouseAttributes().transform(df, drop_cols, target_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 5} 0.690666524719\n",
      "0.751774480166\n",
      "40851.1893427\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'max_depth': [3, 5, 8]}\n",
    "dt = DecisionTreeRegressor()\n",
    "gs_dt = GridSearchCV(dt, param_grid, cv=5)\n",
    "gs_dt.fit(X_train, y_train)\n",
    "print(gs_dt.best_params_, gs_dt.best_score_)\n",
    "\n",
    "y_pred = gs_dt.predict(X_test)\n",
    "print(gs_dt.score(X_test, y_test))\n",
    "print(np.sqrt(mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoosting $_{[1]}$\n",
    "\n",
    "#### Bagging vs. AdaBoosting\n",
    "In bagging, each training example is equally likely to be picked. In boosting, the probability of a particular example being in the training set of a particular machine depends on the performance of the prior machines on that sample. In each round, each training samples is assigned a new weight depending on the prediction performance from the previous round. More weights are assigned to samples with worse prediction so that each subsequent machine would focus on training the \"difficult\" samples.\n",
    "\n",
    "#### Algorithm (Regression)\n",
    "Given $(x_1, y_1),...,(x_n, y_n)$, assign a weight $w_i=1$ for $i = 1,...,n$.\n",
    "\n",
    "For $t = 1,..., T$\n",
    "1. The probability that training sample $i$ is in the training set is $p_i = \\frac{w_i}{\\sum w_i}$ where the summation is over all members of the training set. Pick $n$ samples with replacement to form the training set.\n",
    "2. Construct a regression machine $t$.\n",
    "3. Make prediction $y_i^{(p)}(x_i)$ for $i=1,...,n$ with machine $t$. **Note: $y_i^{(p)}(x_i)$ is the prediction of this machine but not the final prediction of the ensemble model.**\n",
    "4. Calculate a loss for $y_i^{(p)}(x_i)$ and $y_i$. The loss function may be of any functional form as long as $L \\in$ [0,1]. If we let $$ D = sup|y_i^{(p)}(x_i)-y_i|, \\ \\ i = 1,...,n,$$ which means D is the largest error, then we have three candidate loss functions:\n",
    "$$L_i=\\frac{|y_i^{(p)}(x_i)-y_i|}{D} \\textit{(linear)}$$\n",
    "$$L_i=\\frac{|y_i^{(p)}(x_i)-y_i|^2}{D^2} \\textit{(square law)}$$\n",
    "$$L_i=1-\\exp\\left[\\frac{-|y_i^{(p)}(x_i)-y_i|}{D}\\right] \\textit{(exponential)}$$\n",
    "5. Calculate aveage loss $\\overline{L}=\\sum_{i=1}^{n}L_ip_i$\n",
    "6. Form $\\beta=\\frac{\\overline{L}}{1-\\overline{L}}$. $\\beta$ is a measure of confidence in the predictor. Low $\\beta$ means high confidence in the prediction.\n",
    "7. Update the weights: $w_i \\rightarrow w_i\\beta^{[1-L_i]}$. The smaller the loss, the more weight is reduced , making the sample less likely to be picked in the next round.\n",
    "8. For a particular input $x_i$, each of the $T$ machines makes a prediction $h_t, t=1,...,T$. $h_f$ is cumulative prediction using the $T$ predictors:\n",
    "$$h_f = inf\\bigg\\{y \\in Y: \\sum_{t:h_t\\leq y}log(\\frac{1}{\\beta_t})\\geq\\frac{1}{2}\\sum_{t}log(\\frac{1}{\\beta_t})\\bigg\\}$$\n",
    "This equation is essentially relabel $y_i$ such that $y_i^{(1)}<y_i^{(2)}<,...,y_i^{(T)}$. Then sum the $log(1/\\beta_t)$ until we reach the smallest t that is equal or grater than $\\frac{1}{2}\\sum_{t}log(\\frac{1}{\\beta_t})$. This is the **weighted median**. If the $\\beta_t$ were all equal, this would be the median.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abr_3 {'loss': 'exponential', 'n_estimators': 250} 0.78948529636\n",
      "abr_5 {'loss': 'linear', 'n_estimators': 250} 0.812257334853\n",
      "abr_8 {'loss': 'square', 'n_estimators': 250} 0.827129375149\n",
      "Wall time: 6min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "param_grid = {'n_estimators': [250],\n",
    "              'loss' : ['linear', 'square', 'exponential']}\n",
    "abr_3 = AdaBoostRegressor(DecisionTreeRegressor(max_depth=3))\n",
    "abr_5 = AdaBoostRegressor(DecisionTreeRegressor(max_depth=5))\n",
    "abr_8 = AdaBoostRegressor(DecisionTreeRegressor(max_depth=8))\n",
    "for k, v in zip(['abr_3', 'abr_5', 'abr_8'], [abr_3, abr_5, abr_8]):\n",
    "    gs_cv = GridSearchCV(v, param_grid, cv = 5)\n",
    "    gs_cv.fit(X_train, np.ravel(y_train))\n",
    "    print(k, gs_cv.best_params_, gs_cv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared: 0.898281009521 RMSE: 26150.6513963\n"
     ]
    }
   ],
   "source": [
    "y_pred = gs_cv.predict(X_test)\n",
    "print('R-squared:', gs_cv.score(X_test, y_test), 'RMSE:', np.sqrt(mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Machine (GBM) $_{[2]}$\n",
    "\n",
    "#### AdaBoost vs GBM\n",
    "As oppose to AdaBoost which takes $\\textit{the weighted median}$ of predictions from ensembled regressions, GBM boosting is an $\\textit{additive model}$ which adds up predicted values from ensembled regressions. \n",
    "\n",
    "#### General Form\n",
    "The general form of gredient tree-boosting algorithm for regression could be expressed as\n",
    "$$f(x) = \\sum_{m=1}^{M}\\beta_mb(x;\\gamma_m),$$\n",
    "where $\\beta_m, m = 1,2,...,M$ are the expansion coefficients, and $b(x;\\gamma)\\in\\mathbb{R}$ are usually simple functions of the multivariate argument $x$, characterized by a set of parameters $\\gamma$. Specific algorithms are objtained by inserting different loss criteria $L(y, f(x))$\n",
    "\n",
    "#### Algorithm\n",
    "1. Initialize $f_0(x) = \\operatorname*{argmin}_\\gamma\\sum_{i=1}^{N} L(y_i,\\gamma)$.\n",
    "2. For $m = 1$ to $M$:\n",
    "    \n",
    "    (a) For $i = 1,2,...,N$ compute\n",
    "    $$r_{im} = -\\bigg[\\frac{\\partial L(y_i, f(x_i))}{\\partial f(x_i)}\\bigg]_{f=f_{m-1}}.$$\n",
    "    \n",
    "    $r$ is referred as generalized or $\\textit{pseudo}$ residuals. If we set the loss function as $\\frac{1}{2}[y_i - f(x_i)]^2$, the gradient will be $y_i - f(x_i)$. This step is also where the name of the model is from.\n",
    "    \n",
    "    (b) Fit a regression tree to target $r_{im}$ giving terminal regions $R_{jm}, j = 1,2,...,J_m$. \n",
    "    \n",
    "    Here we fit $r$, not $y$. $J$ is the number of leafs. Each iteration $m$ might have different number of leaves $J_m$.\n",
    "    \n",
    "    (c) For $j=1,2,...,J_m$ compute\n",
    "    $$\\gamma_{jm} = \\operatorname*{argmin}_\\gamma\\sum_{x_i\\in R_{jm}}L(y_i, f_{m-1}X_i + \\gamma).$$\n",
    "    \n",
    "    (d) Update $f_m(x) = f_{m-1}(x) + \\sum_{j=1}^{J_m}\\gamma_{jm}I(x\\in R{jm})$.\n",
    "\n",
    "3. Output $\\hat{f}(x) = f_M(x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.14999999999999999, 'max_depth': 3, 'n_estimators': 500} 0.847728415134\n",
      "Wall time: 19min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "param_grid = {'n_estimators': [500],\n",
    "              'learning_rate': np.linspace(0.01, 0.15, 4),\n",
    "              'max_depth': [3, 5, 8]}\n",
    "gbr = GradientBoostingRegressor()\n",
    "gs_cv = GridSearchCV(gbr, param_grid, cv=5)\n",
    "gs_cv.fit(X_train, np.ravel(y_train))\n",
    "print(gs_cv.best_params_, gs_cv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.913572430868 24105.0279711\n"
     ]
    }
   ],
   "source": [
    "y_pred = gs_cv.predict(X_test)\n",
    "print(gs_cv.score(X_test, y_test), np.sqrt(mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost $_{[3]}$\n",
    "\n",
    "#### GBM vs XGBoost\n",
    "Gradient boosting machine, or gradient tree boosting, has been shown to give state-of-the-art results on many standard classification benchmarks, and XGBoost is a scalable machine learning system for tree boosting. The impact of XGBoost has been widely recognized in a number of machine learning and data mining challenges. The regularization term added after the loss function in XGBoost prevents over fitting. This approach improves generalization from training data and increases prediction performance for regression, ranking, and classification problems.\n",
    "\n",
    "Moreover, XGBoost systerm runs more than ten times faster than existing popular solutions on a single machine and scales to billions of examples in distributed or memory-limited setting due to several important systems and algorithmic optimizations, including:\n",
    "- A highly scalable end-to-end tree boosting system.\n",
    "- A theoretically justified weighted quantile sketch for efficient proposal calculation.\n",
    "- A novel sparsity-aware algorithm for parallel tree learning.\n",
    "- An effective cache-aware block structure for out-of-core tree learning.\n",
    "\n",
    "\n",
    "#### Algorithm\n",
    "**Note: The process below is simplified for a brief discussion. For more details please refer to the document [3] listed in references.**\n",
    "\n",
    "The system implements gradient boosting, which performs additive optimization in functional space, and incorporates a regularized model to prevent over fitting.\n",
    "\n",
    "**Gradient Boosting:**\n",
    "\n",
    "$$\\hat{y}_i = \\phi(x_i) = \\sum_{k=1}^{K}f_k(x_i), f_k\\in\\textit{F},$$\n",
    "\n",
    "where $\\textit{F}={f(x)=w_{q(x)}}(q:\\mathbb{R}^m \\rightarrow T, w \\in \\mathbb{R}^T)$ is the space of regression trees (also known as CART). Here $q$ represents the structure of each tree that maps an example to the corresponding leaf index. $T$ is the number of leaves in the tree. Each $f_k$ corresponds to an independent tree structure $q$ and leaf weight $w$. We use $w_i$ to represent score on $i$-th leaf.\n",
    "\n",
    "**XGBoost then minimize the following loss function with $\\textit{regularized}$ objective:** \n",
    "\n",
    "$$\\textit{L}(\\phi) = \\sum_{i}l(\\hat{y}_i, y_i)+\\sum_{k}\\Omega(f_k),$$\n",
    "$$where \\ \\ \\Omega(f) = \\gamma T + \\frac{1}{2}\\lambda\\|w\\|^2$$\n",
    "\n",
    "In addictive manner, let $\\hat{y}_i^t$ be the prediction of the $i$-th instance at the $t$-th iteration, we will need to add $f_t$ to minimize:\n",
    "\n",
    "$$\\textit{L}^{(t)} = \\sum_{i=1}^{n}l(y_i, \\hat{y}_i^{t-1}+f_t(x_i))+\\sum_{k}\\Omega(f_k),$$\n",
    "\n",
    "Apply second-order approximation to quickly optimize the objective and then remove the constant.\n",
    "$$ L^{(t)} \\simeq \\sum_{i=1}^{n}[l(y_i, \\hat{y}_i^{t-1}) + g_if_t(x_i) + \\frac{1}{2}h_if_t^2(x_i)] + \\Omega(f_t) $$\n",
    "$$ = \\sum_{i=1}^{n}[g_if_t(x_i) + \\frac{1}{2}h_if_t^2(x_i)] + \\Omega(f_t), $$\n",
    "where $g_i = \\partial_{\\hat{y}^{(t-1)}}l(y_i,\\hat{y}^{(t-1)})$ and $h_i = \\partial_{\\hat{y}^{(t-1)}}^2l(y_i,\\hat{y}^{(t-1)})$\n",
    "\n",
    "Define $I_j = \\{i|q(x_i)=j\\}$ as the instance set of leaf $j$ and expand $\\Omega$.\n",
    "$$ \\tilde{L}^{(t)} = \\sum_{i=1}^{n}[g_if_t(x_i) + \\frac{1}{2}h_if_t^2(x_i)]+ \\gamma T + \\frac{1}{2}\\lambda\\sum_{j=1}^{T}w_j^2$$\n",
    "$$ = \\sum_{j=1}^{T}[(\\sum_{i \\in I_j}g_i)w_j + \\frac{1}{2}(\\sum_{i\\in I_j}h_i+\\lambda)w_j^2]+\\gamma T$$\n",
    "\n",
    "For a fixed structure $q(x)$, we can compute the optimal weight $w_j^*$ of leaf $j$ by\n",
    "$$ w_j^* = -\\frac{\\sum_{i\\in I_j}g_i}{\\sum_{i \\in I_j}h_i + \\lambda} $$\n",
    "\n",
    "and calculate the corresponding optimal value by\n",
    "$$ \\tilde{L}^{(t)}(q) = -\\frac{1}{2}\\sum_{j=1}^{T}\\frac{(\\sum_{i \\in I_j} g_i)^2}{\\sum_{i \\in I_j} h_i+\\lambda}+\\gamma T $$\n",
    "\n",
    "Assume that $I_L$ and $I_R$ are the instance sets of left and right nodes after the split. Letting $I=I_L \\cup I_R$, then the loss reduction after the split is given by\n",
    "$$ L_{split} = \\frac{1}{2} \\bigg[ \\frac{(\\sum_{i \\in I_L} g_i)^2}{\\sum_{i \\in I_L} h_i+\\lambda} +\n",
    "                                  \\frac{(\\sum_{i \\in I_R} g_i)^2}{\\sum_{i \\in I_R} h_i+\\lambda} -\n",
    "                                  \\frac{(\\sum_{i \\in I} g_i)^2}{\\sum_{i \\in I} h_i+\\lambda} \\bigg] - \\gamma $$\n",
    "\n",
    "This formula is usually used in practice for evaluating the split candicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A new data cleaning process without fill NA as XGBoost can handel missing value\n",
    "class CleanHouseDataForXGBoost(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Apply rules during data exploration to clean house price dataset\"\"\"\n",
    "    \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df_house_price, cols_to_drop, target_col=[]):\n",
    "        # Age of building/remodle from YearBuilt and YearRemodAdd\n",
    "        df_house_price['AgeBuilding'] = 2012 - df_house_price['YearBuilt']\n",
    "        df_house_price['AgeRemodel'] = 2012 - df_house_price['YearRemodAdd']\n",
    "        \n",
    "        # Create boolean variables for Alley, PoolQC, and Fence\n",
    "        df_house_price['HasAlley'] = ~df_house_price[['Alley']].isnull()\n",
    "        df_house_price['HasPool'] = ~df_house_price[['PoolQC']].isnull()\n",
    "        df_house_price['Fence'] = ~df_house_price[['Fence']].isnull()\n",
    "\n",
    "        # Remove categories not exist in test.csv\n",
    "        df_filtered = df_house_price[(df_house_price['HouseStyle'] != '2.5Fin') &\n",
    "                                     (~df_house_price['RoofMatl'].isin(['Membran', 'Roll', 'ClyTile', 'Metal'])) &\n",
    "                                     (~df_house_price['Exterior1st'].isin(['Stone', 'ImStucc'])) &\n",
    "                                     (df_house_price['Exterior2nd'] != 'Other') &\n",
    "                                     (df_house_price['Electrical'] != 'Mix')]\n",
    "        \n",
    "        # Drop columns\n",
    "        df_dropped = df_filtered.drop(cols_to_drop + target_col + ['HasAlley', 'HasPool', 'Fence'], axis=1)\n",
    "               \n",
    "        # Get numeric columns\n",
    "        df_numeric = df_dropped.select_dtypes(include=['int64', 'float64'])\n",
    "        \n",
    "        # Get non-numcric columns\n",
    "        df_nonNumeric = df_dropped.select_dtypes(include=['object'])\n",
    "                \n",
    "        X = pd.concat([df_numeric, pd.get_dummies(df_nonNumeric)], axis=1)\n",
    "        y = df_filtered[target_col]\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = CleanHouseDataForXGBoost().transform(df, drop_cols, target_col)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.056666666666666664, 'max_depth': 3, 'n_estimators': 500} 0.827793626636\n",
      "Wall time: 13min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Use the same parameter as GBM\n",
    "#data_dmatrix = xgb.DMatrix(data = X_train, label = y_train)\n",
    "param_grid = {'max_depth': [3, 5, 8],\n",
    "              'learning_rate': np.linspace(0.01, 0.15, 4),\n",
    "              'n_estimators': [500]}\n",
    "gs_cv = GridSearchCV(xgb.XGBRegressor(), param_grid, cv=5)\n",
    "gs_cv.fit(X_train, y_train)\n",
    "print(gs_cv.best_params_, gs_cv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.899172022077 26035.8655096\n"
     ]
    }
   ],
   "source": [
    "y_pred = gs_cv.predict(X_test)\n",
    "print(gs_cv.score(X_test, y_test), np.sqrt(mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**With the same settings in grid search of parameters, XGBoost runs about 40% faster than GBM. The predicting results on the test set shows that XGBoost seems to generalize the model better as well.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Randomized Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.14999999999999999, 'max_depth': 3, 'n_estimators': 500} -1051860217.83\n",
      "Wall time: 3min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "params = {'max_depth': [3, 5, 8],\n",
    "          'learning_rate': np.linspace(0.01, 0.15, 4),\n",
    "          'n_estimators': [500]}\n",
    "\n",
    "random_search = RandomizedSearchCV(xgb.XGBRegressor(), params, n_iter=5, scoring='neg_mean_squared_error')\n",
    "random_search.fit(X_train, y_train)\n",
    "print(random_search.best_params_, random_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-692245392.488 26310.5566739\n"
     ]
    }
   ],
   "source": [
    "y_pred = random_search.predict(X_test)\n",
    "print(random_search.score(X_test, y_test), np.sqrt(mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Althought it's much faster to use random search, the result is not quite as good as grid search.\n",
    "\n",
    "#### More Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'min_child_weight': 10, 'max_depth': 3, 'n_estimators': 800, 'subsample': 0.6, 'learning_rate': 0.0086, 'gamma': 1, 'colsample_bytree': 0.8} -778456612.689\n",
      "Wall time: 2min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Grid Search\n",
    "params = {'max_depth': [3, 5, 7],\n",
    "          'learning_rate': np.linspace(0.001, 0.020, 11),\n",
    "          'n_estimators': [250, 500, 800],\n",
    "          'min_child_weight': [5, 8, 10],\n",
    "          'gamma': [0.5, 1, 1.5],\n",
    "          'subsample': [0.6, 0.8, 1.0],\n",
    "          'colsample_bytree': [0.6, 0.8, 1.0]\n",
    "          }\n",
    "\n",
    "random_search = RandomizedSearchCV(xgb.XGBRegressor(), params, n_iter=5, scoring='neg_mean_squared_error')\n",
    "random_search.fit(X_train, y_train)\n",
    "print(random_search.best_params_, random_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-694890265.687 26360.7713409\n"
     ]
    }
   ],
   "source": [
    "y_pred = random_search.predict(X_test)\n",
    "print(random_search.score(X_test, y_test), np.sqrt(mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance on Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# GBM: 0.12984\n",
    "X, y = CleanHouseAttributes().transform(df, drop_cols, target_col)\n",
    "X_test_kaggle, y_test_kaggle = CleanHouseAttributes().transform(df_test, drop_cols)\n",
    "\n",
    "param_grid = {'n_estimators': [500],\n",
    "              'learning_rate': np.linspace(0.01, 0.15, 4),\n",
    "              'max_depth': [3, 5]}\n",
    "gbr = GradientBoostingRegressor()\n",
    "gs_cv = GridSearchCV(gbr, param_grid, cv=5)\n",
    "gs_cv.fit(X, np.ravel(y))\n",
    "\n",
    "y_pred_kaggle = gs_cv.predict(X_test_kaggle)\n",
    "pd.DataFrame({'Id': df_test.Id.values,\n",
    "              'SalePrice': np.squeeze(y_pred_kaggle)}).to_csv('../project-house-price-prediction/data/pred.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 37min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# XGBoost: 0.12845\n",
    "X, y = CleanHouseDataForXGBoost().transform(df, drop_cols, target_col)\n",
    "X_test_kaggle, y_test_kaggle = CleanHouseDataForXGBoost().transform(df_test, drop_cols)\n",
    "\n",
    "params_grid = {'max_depth': [3, 5],\n",
    "               'learning_rate': np.linspace(0.001, 0.015, 4),\n",
    "               'n_estimators': [500],\n",
    "               #'min_child_weight': [5, 8, 10],\n",
    "               #'gamma': [0.5, 1, 1.5],\n",
    "               'subsample': [0.6, 0.8],\n",
    "               'colsample_bytree': [0.6, 0.8]\n",
    "              }\n",
    "\n",
    "gs_cv = GridSearchCV(xgb.XGBRegressor(), params_grid, cv=5)\n",
    "gs_cv.fit(X, np.ravel(y))\n",
    "y_pred_kaggle = gs_cv.predict(X_test_kaggle)\n",
    "pd.DataFrame({'Id': df_test.Id.values,\n",
    "              'SalePrice': np.squeeze(y_pred_kaggle)}).to_csv('../project-house-price-prediction/data/pred.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# XGBoost: 0.12628 (top 33.6%)\n",
    "X, y = CleanHouseDataForXGBoost().transform(df, drop_cols, target_col)\n",
    "X_test_kaggle, y_test_kaggle = CleanHouseDataForXGBoost().transform(df_test, drop_cols)\n",
    "\n",
    "params = {'max_depth': [3, 5, 7],\n",
    "          'learning_rate': np.linspace(0.001, 0.020, 11),\n",
    "          'n_estimators': [250, 500, 800],\n",
    "          'min_child_weight': [5, 8, 10],\n",
    "          'gamma': [0.5, 1, 1.5],\n",
    "          'subsample': [0.6, 0.8, 1.0],\n",
    "          'colsample_bytree': [0.6, 0.8, 1.0]\n",
    "          }\n",
    "\n",
    "random_search = RandomizedSearchCV(xgb.XGBRegressor(), params, n_iter=5, scoring='neg_mean_squared_error')\n",
    "random_search.fit(X, np.ravel(y))\n",
    "y_pred_kaggle = random_search.predict(X_test_kaggle)\n",
    "pd.DataFrame({'Id': df_test.Id.values,\n",
    "              'SalePrice': np.squeeze(y_pred_kaggle)}).to_csv('../project-house-price-prediction/data/pred.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'min_child_weight': 5, 'max_depth': 7, 'n_estimators': 800, 'subsample': 0.8, 'learning_rate': 0.0067000000000000002, 'gamma': 1, 'colsample_bytree': 0.8}\n"
     ]
    }
   ],
   "source": [
    "print(random_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "1. H. Drucker, “Improving Regressors using Boosting Techniques”, 1997.\n",
    "2. T. Hastie, R. Tibshirani and J. Friedman, Elements of Statistical Learning Ed. 2, Springer, 2009.\n",
    "3. Tianqi Chen and Carlos Guestrin, \"XGBoost: A Scalable Tree Boosting System\", 2016."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
