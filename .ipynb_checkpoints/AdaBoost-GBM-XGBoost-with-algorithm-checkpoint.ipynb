{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# House Price Prediction\n",
    "\n",
    "\n",
    "## Boosting\n",
    "\n",
    "This notebook discusses the algorithm and application of boosting methods for better predicting performance than single models.\n",
    "\n",
    "## Overview    \n",
    "- Data\n",
    "- Models\n",
    "    - Decision Tree\n",
    "    - AdaBoosting\n",
    "        - Bagging vs. AdaBoosting\n",
    "        - Algorithm (Regression)\n",
    "    - Gradient Boosting Machine (GBM)\n",
    "        - AdaBoost vs GBM\n",
    "        - General Form\n",
    "        - Algorithm\n",
    "    - XGBoost\n",
    "        - GBM vs XGBoost\n",
    "        - Algorithm\n",
    "- Performance on Kaggle\n",
    "    - GradientBoostingRegressor from sklearn\n",
    "    - XGBoost from xgboost\n",
    "    - Executive Summary\n",
    "- References\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read in the data\n",
    "df = pd.read_csv('../project-house-price-prediction/data/train.csv')\n",
    "df_test = pd.read_csv('../project-house-price-prediction/data/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "***The data preparation below follows the process done in the seperate notebook data-exploration-and-preprocessing under the same repository.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HouseDataFeatureEngineering(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Apply rules during data exploration to clean house price dataset\"\"\"\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        # Numerical Features:\n",
    "            # Age of building/remodle from YearBuilt and YearRemodAdd\n",
    "        if np.all((X.dtypes == 'int64') | (X.dtypes == 'float64')):\n",
    "            X = X.assign(AgeBuilding = 2012 - X['YearBuilt'],\n",
    "                         AgeRemodel = 2012 - X['YearRemodAdd'])\n",
    "\n",
    "            X.drop(['YearBuilt', 'YearRemodAdd'], axis=1, inplace=True)\n",
    "        \n",
    "        # Non-numerical Features:\n",
    "            # Create boolean variables for Alley, PoolQC, and Fence\n",
    "        if np.all(X.dtypes == 'object'):\n",
    "            X = X.assign(HasAlley = [False if x is None else True for x in X.Alley],\n",
    "                            HasPool = [False if x is None else True for x in X.PoolQC],\n",
    "                            Fence = [False if x is None else True for x in X.Fence])\n",
    "            X.drop(['Alley', 'PoolQC', 'Fence'], axis=1, inplace=True)\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HouseDataDropColumns(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Apply rules during data exploration to clean house price dataset\"\"\"\n",
    "    \n",
    "    def __init__(self, drop_columns=['Id', 'GarageYrBlt', 'TotalBsmtSF', 'TotRmsAbvGrd', \n",
    "                                     'MoSold', 'YrSold', 'GarageQual', 'Street', 'Utilities', \n",
    "                                     'LandSlope', 'Condition2', 'Heating', 'Functional', \n",
    "                                     'FireplaceQu', 'MiscFeature']):\n",
    "        self.drop_columns = drop_columns        \n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        # To apply the transformer on numeric and non-numeric data frames respectively\n",
    "        drop_it = [x for x in self.drop_columns if x in X.columns]\n",
    "        X.drop(drop_it, axis=1, inplace=True)\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build preprocessing pipeline by numeric and non-numeric columns\n",
    "numeric_features = df.drop(['SalePrice'], axis=1\n",
    "                          ).select_dtypes(include=['int64', 'float64']).columns\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('drop', HouseDataDropColumns()),\n",
    "    ('feature', HouseDataFeatureEngineering()),\n",
    "    ('imputer', SimpleImputer(strategy='median'))])\n",
    "\n",
    "categorical_features = df.select_dtypes(include=['object']).columns\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('drop', HouseDataDropColumns()),\n",
    "    ('feature', HouseDataFeatureEngineering()),\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['SalePrice'], axis = 1)\n",
    "y = df[['SalePrice']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "### Decision Tree\n",
    "From the notebook models-and-algorithm under the same repository, it's identified that decision tree has the lowest performance among other single regression models. Therefore, decision tree is selected here to observe the performace improvement after boosting methods and to compare with gradient boosting tree models that will be introduced later in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dt__max_depth': 8} 0.5894056896304389\n",
      "0.6885879692663159\n",
      "44150.27139297183\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'dt__max_depth': [3, 5, 8]}\n",
    "dt = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                     ('dt', DecisionTreeRegressor())])\n",
    "gs_dt = GridSearchCV(dt, param_grid, cv=5)\n",
    "gs_dt.fit(X_train, y_train)\n",
    "print(gs_dt.best_params_, gs_dt.best_score_)\n",
    "\n",
    "y_pred = gs_dt.predict(X_test)\n",
    "print(gs_dt.score(X_test, y_test))\n",
    "print(np.sqrt(mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoosting $_{[1]}$\n",
    "\n",
    "#### Bagging vs. AdaBoosting\n",
    "In bagging, each training example is equally likely to be picked. In boosting, the probability of a particular example being in the training set of a particular machine depends on the performance of the prior machines on that sample. In each round, each training samples is assigned a new weight depending on the prediction performance from the previous round. More weights are assigned to samples with worse prediction so that each subsequent machine would focus on training the \"difficult\" samples.\n",
    "\n",
    "#### Algorithm (Regression)\n",
    "Given $(x_1, y_1),...,(x_n, y_n)$, assign a weight $w_i=1$ for $i = 1,...,n$.\n",
    "\n",
    "For $t = 1,..., T$\n",
    "1. The probability that training sample $i$ is in the training set is $p_i = \\frac{w_i}{\\sum w_i}$ where the summation is over all members of the training set. Pick $n$ samples with replacement to form the training set.\n",
    "2. Construct a regression machine $t$.\n",
    "3. Make prediction $y_i^{(p)}(x_i)$ for $i=1,...,n$ with machine $t$. **Note: $y_i^{(p)}(x_i)$ is the prediction of this machine but not the final prediction of the ensemble model.**\n",
    "4. Calculate a loss for $y_i^{(p)}(x_i)$ and $y_i$. The loss function may be of any functional form as long as $L \\in$ [0,1]. If we let $$ D = sup|y_i^{(p)}(x_i)-y_i|, \\ \\ i = 1,...,n,$$ which means D is the largest error, then we have three candidate loss functions:\n",
    "$$L_i=\\frac{|y_i^{(p)}(x_i)-y_i|}{D} \\ \\ \\ \\textit{(linear)}$$\n",
    "$$L_i=\\frac{|y_i^{(p)}(x_i)-y_i|^2}{D^2} \\ \\ \\ \\textit{(square law)}$$\n",
    "$$L_i=1-\\exp\\left[\\frac{-|y_i^{(p)}(x_i)-y_i|}{D}\\right] \\ \\ \\ \\textit{(exponential)}$$\n",
    "5. Calculate aveage loss $\\overline{L}=\\sum_{i=1}^{n}L_ip_i$\n",
    "6. Form $\\beta=\\frac{\\overline{L}}{1-\\overline{L}}$. $\\beta$ is a measure of confidence in the predictor. Low $\\beta$ means high confidence in the prediction.\n",
    "7. Update the weights: $w_i \\rightarrow w_i\\beta^{[1-L_i]}$. The smaller the loss, the more weight is reduced , making the sample less likely to be picked in the next round.\n",
    "8. For a particular input $x_i$, each of the $T$ machines makes a prediction $h_t, t=1,...,T$. $h_f$ is cumulative prediction using the $T$ predictors:\n",
    "$$h_f = inf\\bigg\\{y \\in Y: \\sum_{t:h_t\\leq y}log(\\frac{1}{\\beta_t})\\geq\\frac{1}{2}\\sum_{t}log(\\frac{1}{\\beta_t})\\bigg\\}$$\n",
    "This equation is essentially relabel $y_i$ such that $y_i^{(1)}<y_i^{(2)}<,...,y_i^{(T)}$. Then sum the $log(1/\\beta_t)$ until we reach the smallest t that is equal or grater than $\\frac{1}{2}\\sum_{t}log(\\frac{1}{\\beta_t})$. This is the **weighted median**. If the $\\beta_t$ were all equal, this would be the median.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abr_3 {'abr__loss': 'exponential', 'abr__n_estimators': 250} 0.713203389204278\n",
      "abr_5 {'abr__loss': 'linear', 'abr__n_estimators': 250} 0.765606456356722\n",
      "abr_8 {'abr__loss': 'square', 'abr__n_estimators': 250} 0.7441814079195108\n",
      "Wall time: 10min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "param_grid = {'abr__n_estimators': [250],\n",
    "              'abr__loss' : ['linear', 'square', 'exponential']}\n",
    "abr_3 = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                        ('abr', AdaBoostRegressor(DecisionTreeRegressor(max_depth=3)))])\n",
    "abr_5 = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                        ('abr', AdaBoostRegressor(DecisionTreeRegressor(max_depth=5)))])\n",
    "abr_8 = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                        ('abr', AdaBoostRegressor(DecisionTreeRegressor(max_depth=8)))])\n",
    "for k, v in zip(['abr_3', 'abr_5', 'abr_8'], [abr_3, abr_5, abr_8]):\n",
    "    gs_cv = GridSearchCV(v, param_grid, cv = 5)\n",
    "    gs_cv.fit(X_train, np.ravel(y_train))\n",
    "    print(k, gs_cv.best_params_, gs_cv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared: 0.8906824177427002 RMSE: 26158.37089278115\n"
     ]
    }
   ],
   "source": [
    "y_pred = gs_cv.predict(X_test)\n",
    "print('R-squared:', gs_cv.score(X_test, y_test), \n",
    "      'RMSE:', np.sqrt(mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction on test set has improved quite much after applying AdaBoosting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Machine (GBM) $_{[2]}$\n",
    "\n",
    "#### AdaBoost vs GBM\n",
    "As oppose to AdaBoost which takes $\\textit{the weighted median}$ of predictions from ensembled regressions, GBM boosting is an $\\textit{additive model}$ which adds up predicted values from ensembled regressions. \n",
    "\n",
    "#### General Form\n",
    "The general form of gredient tree-boosting algorithm for regression could be expressed as\n",
    "$$f(x) = \\sum_{m=1}^{M}\\beta_mb(x;\\gamma_m),$$\n",
    "where $\\beta_m, m = 1,2,...,M$ are the expansion coefficients, and $b(x;\\gamma)\\in\\mathbb{R}$ are usually simple functions of the multivariate argument $x$, characterized by a set of parameters $\\gamma$. Specific algorithms are objtained by inserting different loss criteria $L(y, f(x))$\n",
    "\n",
    "#### Algorithm\n",
    "1. Initialize $f_0(x) = \\operatorname*{argmin}_\\gamma\\sum_{i=1}^{N} L(y_i,\\gamma)$.\n",
    "2. For $m = 1$ to $M$:\n",
    "    \n",
    "    (a) For $i = 1,2,...,N$ compute\n",
    "    $$r_{im} = -\\bigg[\\frac{\\partial L(y_i, f(x_i))}{\\partial f(x_i)}\\bigg]_{f=f_{m-1}}.$$\n",
    "    \n",
    "    $r$ is referred as generalized or $\\textit{pseudo}$ residuals. If we set the loss function as $\\frac{1}{2}[y_i - f(x_i)]^2$, the gradient will be $y_i - f(x_i)$. This step is also where the name of the model is from.\n",
    "    \n",
    "    (b) Fit a regression tree to target $r_{im}$ giving terminal regions $R_{jm}, j = 1,2,...,J_m$. \n",
    "    \n",
    "    Here we fit $r$, not $y$. $J$ is the number of leafs. Each iteration $m$ might have different number of leaves $J_m$.\n",
    "    \n",
    "    (c) For $j=1,2,...,J_m$ compute\n",
    "    $$\\gamma_{jm} = \\operatorname*{argmin}_\\gamma\\sum_{x_i\\in R_{jm}}L(y_i, f_{m-1}X_i + \\gamma).$$\n",
    "    \n",
    "    (d) Update $f_m(x) = f_{m-1}(x) + \\sum_{j=1}^{J_m}\\gamma_{jm}I(x\\in R{jm})$.\n",
    "\n",
    "3. Output $\\hat{f}(x) = f_M(x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gbr__max_depth': 3, 'gbr__n_estimators': 500, 'gbr__learning_rate': 0.056666666666666664} 0.7438394689713702\n",
      "Wall time: 21min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "param_grid = {'gbr__n_estimators': [500],\n",
    "              'gbr__learning_rate': np.linspace(0.01, 0.15, 4),\n",
    "              'gbr__max_depth': [3, 5, 8]}\n",
    "gbr = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('gbr', GradientBoostingRegressor())])\n",
    "gs_cv = GridSearchCV(gbr, param_grid, cv=5)\n",
    "gs_cv.fit(X_train, np.ravel(y_train))\n",
    "print(gs_cv.best_params_, gs_cv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9005000689587325 24956.12051050492\n"
     ]
    }
   ],
   "source": [
    "y_pred = gs_cv.predict(X_test)\n",
    "print(gs_cv.score(X_test, y_test), np.sqrt(mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient boosting regression generates better results than AdaBoosting. However, speed is the main issue to run more parameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost $_{[3]}$\n",
    "\n",
    "#### GBM vs XGBoost\n",
    "Gradient boosting machine, or gradient tree boosting, has been shown to give state-of-the-art results on many standard classification benchmarks, and XGBoost is a scalable machine learning system for tree boosting. The impact of XGBoost has been widely recognized in a number of machine learning and data mining challenges. The regularization term added after the loss function in XGBoost prevents over fitting. This approach improves generalization from training data and increases prediction performance for regression, ranking, and classification problems.\n",
    "\n",
    "Moreover, XGBoost systerm runs more than ten times faster than existing popular solutions on a single machine and scales to billions of examples in distributed or memory-limited setting due to several important systems and algorithmic optimizations, including:\n",
    "- A highly scalable end-to-end tree boosting system.\n",
    "- A theoretically justified weighted quantile sketch for efficient proposal calculation.\n",
    "- A novel sparsity-aware algorithm for parallel tree learning.\n",
    "- An effective cache-aware block structure for out-of-core tree learning.\n",
    "\n",
    "\n",
    "#### Algorithm\n",
    "**Note: The process below is simplified for a brief discussion. For more details please refer to the document [3] listed in references.**\n",
    "\n",
    "The system implements gradient boosting, which performs additive optimization in functional space, and incorporates a regularized model to prevent over fitting.\n",
    "\n",
    "**Gradient Boosting:**\n",
    "\n",
    "$$\\hat{y}_i = \\phi(x_i) = \\sum_{k=1}^{K}f_k(x_i), f_k\\in\\textit{F},$$\n",
    "\n",
    "where $\\textit{F}={f(x)=w_{q(x)}}(q:\\mathbb{R}^m \\rightarrow T, w \\in \\mathbb{R}^T)$ is the space of regression trees (also known as CART). Here $q$ represents the structure of each tree that maps an example to the corresponding leaf index. $T$ is the number of leaves in the tree. Each $f_k$ corresponds to an independent tree structure $q$ and leaf weight $w$. We use $w_i$ to represent score on $i$-th leaf.\n",
    "\n",
    "**XGBoost then minimize the following loss function with $\\textit{regularized}$ objective:** \n",
    "\n",
    "$$\\textit{L}(\\phi) = \\sum_{i}l(\\hat{y}_i, y_i)+\\sum_{k}\\Omega(f_k),$$\n",
    "$$where \\ \\ \\Omega(f) = \\gamma T + \\frac{1}{2}\\lambda\\|w\\|^2$$\n",
    "\n",
    "In addictive manner, let $\\hat{y}_i^t$ be the prediction of the $i$-th instance at the $t$-th iteration, we will need to add $f_t$ to minimize:\n",
    "\n",
    "$$\\textit{L}^{(t)} = \\sum_{i=1}^{n}l(y_i, \\hat{y}_i^{t-1}+f_t(x_i))+\\sum_{k}\\Omega(f_k),$$\n",
    "\n",
    "Apply second-order approximation to quickly optimize the objective and then remove the constant.\n",
    "$$ L^{(t)} \\simeq \\sum_{i=1}^{n}[l(y_i, \\hat{y}_i^{t-1}) + g_if_t(x_i) + \\frac{1}{2}h_if_t^2(x_i)] + \\Omega(f_t) $$\n",
    "$$ = \\sum_{i=1}^{n}[g_if_t(x_i) + \\frac{1}{2}h_if_t^2(x_i)] + \\Omega(f_t), $$\n",
    "where $g_i = \\partial_{\\hat{y}^{(t-1)}}l(y_i,\\hat{y}^{(t-1)})$ and $h_i = \\partial_{\\hat{y}^{(t-1)}}^2l(y_i,\\hat{y}^{(t-1)})$\n",
    "\n",
    "Define $I_j = \\{i|q(x_i)=j\\}$ as the instance set of leaf $j$ and expand $\\Omega$.\n",
    "$$ \\tilde{L}^{(t)} = \\sum_{i=1}^{n}[g_if_t(x_i) + \\frac{1}{2}h_if_t^2(x_i)]+ \\gamma T + \\frac{1}{2}\\lambda\\sum_{j=1}^{T}w_j^2$$\n",
    "$$ = \\sum_{j=1}^{T}[(\\sum_{i \\in I_j}g_i)w_j + \\frac{1}{2}(\\sum_{i\\in I_j}h_i+\\lambda)w_j^2]+\\gamma T$$\n",
    "\n",
    "For a fixed structure $q(x)$, we can compute the optimal weight $w_j^*$ of leaf $j$ by\n",
    "$$ w_j^* = -\\frac{\\sum_{i\\in I_j}g_i}{\\sum_{i \\in I_j}h_i + \\lambda} $$\n",
    "\n",
    "and calculate the corresponding optimal value by\n",
    "$$ \\tilde{L}^{(t)}(q) = -\\frac{1}{2}\\sum_{j=1}^{T}\\frac{(\\sum_{i \\in I_j} g_i)^2}{\\sum_{i \\in I_j} h_i+\\lambda}+\\gamma T $$\n",
    "\n",
    "Assume that $I_L$ and $I_R$ are the instance sets of left and right nodes after the split. Letting $I=I_L \\cup I_R$, then the loss reduction after the split is given by\n",
    "$$ L_{split} = \\frac{1}{2} \\bigg[ \\frac{(\\sum_{i \\in I_L} g_i)^2}{\\sum_{i \\in I_L} h_i+\\lambda} +\n",
    "                                  \\frac{(\\sum_{i \\in I_R} g_i)^2}{\\sum_{i \\in I_R} h_i+\\lambda} -\n",
    "                                  \\frac{(\\sum_{i \\in I} g_i)^2}{\\sum_{i \\in I} h_i+\\lambda} \\bigg] - \\gamma $$\n",
    "\n",
    "This formula is usually used in practice for evaluating the split candicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'xgbr__max_depth': 3, 'xgbr__n_estimators': 500, 'xgbr__learning_rate': 0.056666666666666664} 0.7461741945197997\n",
      "Wall time: 7min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Use the same parameter as GBM\n",
    "#data_dmatrix = xgb.DMatrix(data = X_train, label = y_train)\n",
    "param_grid = {'xgbr__max_depth': [3, 5, 8],\n",
    "              'xgbr__learning_rate': np.linspace(0.01, 0.15, 4),\n",
    "              'xgbr__n_estimators': [500]}\n",
    "xgbr = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                       ('xgbr', xgb.XGBRegressor())])\n",
    "gs_cv = GridSearchCV(xgbr, param_grid, cv=5)\n",
    "gs_cv.fit(X_train, y_train)\n",
    "print(gs_cv.best_params_, gs_cv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9008914810201432 24906.986044489637\n"
     ]
    }
   ],
   "source": [
    "y_pred = gs_cv.predict(X_test)\n",
    "print(gs_cv.score(X_test, y_test), np.sqrt(mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the same settings in grid search of parameters, XGBoost runs about **65%** faster than GBM. In addition, both R2 and RMSE are both **better from XGBoost model** even though the parameters are the same. This is possibly due to different algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Randomized Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'xgbr__max_depth': 3, 'xgbr__n_estimators': 500, 'xgbr__learning_rate': 0.10333333333333332} -1406074824.6038938\n",
      "Wall time: 3min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "params = {'xgbr__max_depth': [3, 5, 8],\n",
    "          'xgbr__learning_rate': np.linspace(0.01, 0.15, 4),\n",
    "          'xgbr__n_estimators': [500]}\n",
    "xgbr = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                       ('xgbr', xgb.XGBRegressor())])\n",
    "random_search = RandomizedSearchCV(xgbr, params, n_iter=5, \n",
    "                                   scoring='neg_mean_squared_error')\n",
    "random_search.fit(X_train, y_train)\n",
    "print(random_search.best_params_, random_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-618244281.5745714 24864.518526900363\n"
     ]
    }
   ],
   "source": [
    "y_pred = random_search.predict(X_test)\n",
    "print(random_search.score(X_test, y_test), np.sqrt(mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using random search would further save more than half of the tuning time with even better RMSE in final output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'xgbr__subsample': 0.6, 'xgbr__max_depth': 3, 'xgbr__n_estimators': 500, 'xgbr__learning_rate': 0.0181, 'xgbr__gamma': 0.5, 'xgbr__colsample_bytree': 0.8, 'xgbr__min_child_weight': 10} -1108259264.8463147\n",
      "Wall time: 3min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Grid Search\n",
    "params = {'xgbr__max_depth': [3, 5, 7],\n",
    "          'xgbr__learning_rate': np.linspace(0.001, 0.020, 11),\n",
    "          'xgbr__n_estimators': [250, 500, 800],\n",
    "          'xgbr__min_child_weight': [5, 8, 10],\n",
    "          'xgbr__gamma': [0.5, 1, 1.5],\n",
    "          'xgbr__subsample': [0.6, 0.8, 1.0],\n",
    "          'xgbr__colsample_bytree': [0.6, 0.8, 1.0]\n",
    "          }\n",
    "xgbr = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                       ('xgbr', xgb.XGBRegressor())])\n",
    "random_search = RandomizedSearchCV(xgbr, params, n_iter=5, \n",
    "                                   scoring='neg_mean_squared_error')\n",
    "random_search.fit(X_train, y_train)\n",
    "print(random_search.best_params_, random_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-634106579.2699922 25181.472936863567\n"
     ]
    }
   ],
   "source": [
    "y_pred = random_search.predict(X_test)\n",
    "print(random_search.score(X_test, y_test), np.sqrt(mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More parameter tuing is an option for XGBoost with random search as speed doesn't seem to be an issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance on Kaggle\n",
    "### GradientBoostingRegressor from `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 12min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# GBM: 0.13090 1994/5041\n",
    "param_grid = {'gbr__n_estimators': [500],\n",
    "              'gbr__learning_rate': np.linspace(0.01, 0.15, 4),\n",
    "              'gbr__max_depth': [3, 5]}\n",
    "gbr = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('gbr', GradientBoostingRegressor())])\n",
    "gs_cv = GridSearchCV(gbr, param_grid, cv=5)\n",
    "gs_cv.fit(X, np.ravel(y))\n",
    "\n",
    "y_pred_kaggle = gs_cv.predict(df_test)\n",
    "pd.DataFrame({'Id': df_test.Id.values,\n",
    "              'SalePrice': np.squeeze(y_pred_kaggle)}\n",
    "            ).to_csv('../project-house-price-prediction/data/pred.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gbr__max_depth': 3, 'gbr__n_estimators': 500, 'gbr__learning_rate': 0.056666666666666664}\n"
     ]
    }
   ],
   "source": [
    "print(gs_cv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the submissions in notebook models-and-algorithms, the submission of gradient boosting regression on Kaggle is much better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost from `xgboost`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# XGBoost: 0.12391 1495/5047\n",
    "params = {'xgbr__max_depth': [6, 7, 8],\n",
    "          'xgbr__learning_rate': np.linspace(0.001, 0.020, 11),\n",
    "          'xgbr__n_estimators': [500, 800, 1200],\n",
    "          'xgbr__min_child_weight': [10, 12, 14],\n",
    "          'xgbr__gamma': [0, 0.5, 1],\n",
    "          'xgbr__subsample': [0.6, 0.7, 0.8],\n",
    "          'xgbr__colsample_bytree': [0.4, 0.5, 0.6]\n",
    "          }\n",
    "xgbr = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                       ('xgbr', xgb.XGBRegressor())])\n",
    "random_search = RandomizedSearchCV(xgbr, params, n_iter=5, \n",
    "                                   scoring='neg_mean_squared_error')\n",
    "random_search.fit(X, np.ravel(y))\n",
    "y_pred_kaggle = random_search.predict(df_test)\n",
    "pd.DataFrame({'Id': df_test.Id.values,\n",
    "              'SalePrice': np.squeeze(y_pred_kaggle)}\n",
    "            ).to_csv('../project-house-price-prediction/data/pred.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each time the random search might result in different best parameters. Within all trials, the best submission truned out to come from model with the following parameters, with score 0.12391 and ranks top **30%** at the time of submission.\n",
    "\n",
    "- {'xgbr__n_estimators': 1200, 'xgbr__gamma': 0, 'xgbr__learning_rate': 0.0162, 'xgbr__min_child_weight': 12, 'xgbr__subsample': 0.6, 'xgbr__max_depth': 6, 'xgbr__colsample_bytree': 0.5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(random_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executive Summary\n",
    "- Compared to single models, both GradientBoostingRegressor and XGBoost have better generalization and suffer less over-fitting, resulting in better submission scores on Kaggle.\n",
    "- The biggest advantage of using XGBoost is the speed, which allows more trails on parameter tuning to improve model. \n",
    "- In addition, with the same parameters, XGBoost also seems to generate slightly better R-2 and RMSE than GradientBoostingRegressor, possibly thanks to the enhanced algorithm under the hood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "1. H. Drucker, “Improving Regressors using Boosting Techniques”, 1997.\n",
    "2. T. Hastie, R. Tibshirani and J. Friedman, Elements of Statistical Learning Ed. 2, Springer, 2009.\n",
    "3. Tianqi Chen and Carlos Guestrin, \"XGBoost: A Scalable Tree Boosting System\", 2016."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
